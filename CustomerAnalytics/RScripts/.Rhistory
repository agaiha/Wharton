pZonalAnalysis <- ggplot(data = zonalMean.melted, aes(x = Year, y = value, color=variable, group = variable)) +
geom_area(aes( fill= variable), position = 'stack')  +
labs(y = "Deviation (°C )") +
labs(x = "Year") +
labs(title = "Zonal Land-Ocean Temperature Index")+
theme(axis.title.x = element_text(face="bold", colour="#990000", size=16),
axis.text.x  = element_text(size=10, vjust=0.5),
axis.title.y = element_text(angle = 90, face="bold", colour="#990000", size=16),
axis.text.y  = element_text(size=10),
#scale_x_continuous(breaks=c(1880,2015), labels=c("horrible", "ok")), #seq(1880,2020,by=10)),
#axis.ticks = element_blank(),
plot.title = element_text(size=20, colour="#990000")) +
scale_x_discrete(breaks = seq(1880, 2020, 10));
pZonalAnalysis
zonalMean.melted <- melt(zonalMean, id = "Year")
pZonalAnalysis <- ggplot(data = zonalMean.melted, aes(x = Year, y = value, color=variable, group = variable)) +
geom_area(aes( fill= variable), position = 'stack')  +
labs(y = "Deviation (°C )") +
labs(x = "Year") +
labs(title = "Zonal Land-Ocean Temperature Index")+
theme(axis.title.x = element_text(face="bold", colour="#990000", size=16),
axis.text.x  = element_text(size=10, vjust=0.5),
axis.title.y = element_text(face="bold", colour="#990000", size=16),
axis.text.y  = element_text(size=10),
#scale_x_continuous(breaks=c(1880,2015), labels=c("horrible", "ok")), #seq(1880,2020,by=10)),
#axis.ticks = element_blank(),
plot.title = element_text(size=20, colour="#990000")) +
scale_x_discrete(breaks = seq(1880, 2020, 10));
pZonalAnalysis
install.packages("BTYD")
params <- c(1.20, 0.75, 0.66, 2.78)
bgbb.ConditionalExpectedTransactions(params, n.cal=6, n.star=10, x=3, t.x=4)
library(BTYD)
bgbb.ConditionalExpectedTransactions(params, n.cal=6, n.star=10, x=3, t.x=4)
# We can also use vectors as input:
bgbb.ConditionalExpectedTransactions(params, n.cal=6, n.star=1:10, x=3, t.x=4)
bgbb.ConditionalExpectedTransactions(params, n.cal=6, n.star=10, x=1:4, t.x=4)
library(BTYD)
cdnowElog
<-
system.file
(
"data/cdnowElog.csv"
,
package
=
"BTYD"
)
cdnowElog <- system.file("data/cdnowElog.csv", package = "BTYD")
elog <- dc.ReadLines(cdnowElog, cust.idx = 2, date.idx = 3, sales.idx = 5)
elog[1:3,]
elog[1:5,]
elog$date <- as.Date(elog$date, "%Y%m%d")
elog[1:5,]
elog <- dc.MergeTransactionsOnSameDate(elog)
setwd("~/BDA/BusinessAnalytics/CustomerAnalytics/RScripts")
library(BTYD)
cdnowElog <- system.file("data/cdnowElog.csv", package = "BTYD")
elog <- dc.ReadLines(cdnowElog, cust.idx = 2, date.idx = 3, sales.idx = 5)
elog[1:3,]
elog$date <- as.Date(elog$date, "%Y%m%d")
elog[1:3,]
elog <- dc.MergeTransactionsOnSameDate(elog)
elog <- dc.MergeTransactionsOnSameDate(elog)
elog[1:3,]
end.of.cal.period <- as.Date("1997-09-30")
## 30 September 1997 as the cutoff date,
## as this point (39 weeks)divides the dataset in half
end.of.cal.period <- as.Date("1997-09-30")
elog.cal <- elog[which(elog$date <= end.of.cal.period), ]
split.data <- dc.SplitUpElogForRepeatTrans(elog.cal);
clean.elog <- split.data$repeat.trans.elog;
freq.cbt <- dc.CreateFreqCBT(clean.elog);
freq.cbt[1:3,1:5]
freq.cbt[1:3,1:10]
tot.cbt <-dc.CreateFreqCBT(elog)
cal.cbt<-dc.MergeCustomers(tot.cbt, freq.cbt)
birth.periods<-split.data$cust.data$birth.per
last.dates<-split.data$cust.data$last.date
cal.cbs.dates<-data.frame(birth.periods, last.dates,end.of.cal.period)
cal.cbs<-dc.BuildCBSFromCBTAndDates(cal.cbt, cal.cbs.dates,per="week")
cal.cbs<-dc.BuildCBSFromCBTAndDates(cal.cbt, cal.cbs.dates,per="week")
params<-pnbd.EstimateParameters(cal.cbs);
params
LL<-pnbd.cbs.LL(params, cal.cbs);
LL
p.matrix<-c(params, LL);
for (i in 1:2) {
params <- pnbd.EstimateParameters (cal.cbs, params);
LL <- pnbd.cbs.LL (params, cal.cbs);
p.matrix.row <- c(params, LL);
p.matrix <- rbind(p.matrix, p.matrix.row);
}
colnames(p.matrix) <- c("r", "alpha", "s", "beta", "LL");
colnames(p.matrix) <- c("r", "alpha", "s", "beta", "LL");
rownames(p.matrix) <- 1:3;
p.matrix
pnbd.PlotTransactionRateHeterogeneity(params)
## in figure 3, plotted using pnbd.PlotDropoutHeterogeneity(params)
pnbd.PlotDropoutHeterogeneity(params)
pnbd.PlotTransactionRateHeterogeneity(params)
pnbd.PlotDropoutHeterogeneity(params)
pnbd.PlotDropoutHeterogeneity(params)
pnbd.Expectation(params,t=52);
cal.cbs["1516",]
cal.cbs["1516",]
x<- cal.cbs["1516", "x"]
t.x <- cal.cbs["1516", "t.x"]
T.cal <-cal.cbs["1516", "T.cal"]
pnbd.ConditionalExpectedTransactions(params, T.star = 52, x, t.x, T.cal)
pnbd.PAlive(params, x, t.x, T.cal)
for (i in seq( 10, 25, 5)) {
cond.expectation <- pnbd.ConditionalExpectedTransactions(params, T.star = 52,
x = i, t.x = 20,  T.cal =39)
cat("x:" ,i," \t Expectation:", cond.expectation, fill = TRUE )
}
pnbd.PlotFrequencyInCalibration(params, cal.cbs,7)
elog<-  dc.SplitUpElogForRepeatTrans(elog)$repeat.trans.elog;
x.star<-rep(0,nrow(cal.cbs));
cal.cbs<-cbind(cal.cbs, x.star);
elog.custs<-elog$cust
for(i in 1 : nrow (cal.cbs)) {
current.cust  <- rownames(cal.cbs)[i]
tot.cust.trans <- length( which(elog.custs == current.cust))
cal.trans <- cal.cbs[i, "x"]
cal.cbs[i, "x.star"]  <- tot.cust.trans - cal.trans
}
cal.cbs[1 : 3,]
T.star<-  39 # length of the holdout period
censor<-7 # This censor serves the same purpose described above
x.star<-cal.cbs[, "x.s tar"]
x.star<-cal.cbs[, "x.star"]
comp <- pnbd.PlotFreqVsConditionalExpectedFrequency(params, T.star, cal.cbs, x.star, censor)
rownames(comp)<-c("act","exp","bin")
comp
tot.cbt<-  dc.CreateFreqCBT(elog)
tot.cbt<-  dc.CreateFreqCBT(elog)
d.track.data<-rep(0,7*78)
origin<-as.Date("1997-01-01")
for(i in colnames(tot.cbt)) {
date.index  <-  difftime( as.Date(i), origin) + 1;
d.track.data[date.index]<-sum(tot.cbt[,i]);
}
w.track.data<-rep(0,78)
w.track.data<-rep(0,78)
for(j in 1 : 78){
w.track.data[j] <-sum(d.track.data[(j * 7 -6) : (j * 7)])
}
T.cal<- cal.cbs[, "T.cal"]
T.tot<- 78
n.periods.final<- 78
inc.tracking<-pnbd.PlotTrackingInc(params, T.cal,
T.tot, w.track.data,
n.periods.final)
inc.tracking[, 20:25
inc.tracking[, 20:25]
inc.tracking[,20:25]
cum.tracking.data<- cumsum(w.track.data)
cum.tracking<-pnbd.PlotTrackingCum(params, T.cal,
T.tot, cum.tracking.data,
n.periods.final)
cum.tracking[, 20:25]
library(BTYD)
### STEP 1: data preparation
simElog<- system.file("data/discreteSimElog.csv", package="BTYD")
elog<-dc.ReadLines(simElog,cust.idx=1,date.idx= 2)
elog[1: 3,]
elog$date<- as.Date(elog$date,"%Y-%m-%d")
max(elog$date);
max(elog$date)
min(elog$date)
T.cal<-  as.Date("1977-01-01")
T.cal<-  as.Date("1977-01-01")
simData<-dc.ElogToCbsCbt(elog,per="year", T.cal)
cal.cbs<- simData$cal$cbs
freq<-cal.cbs[, "x"]
rec<-cal.cbs[,"t.x"]
trans.opp<- 7
# transaction opportunities
cal.rf.matrix<- dc.MakeRFmatrixCal(freq, rec, trans.opp)
cal.rf.matrix[1:5,]
data(donationsSummary);
rf.matrix<-donationsSummary$rf.matrix
params<-bgbb.EstimateParameters(rf.matrix);
LL<-bgbb.rf.matrix.LL(params, rf.matrix);
p.matrix<-c(params, LL);
for(i in 1: 2) {
params<-bgbb.EstimateParameters(rf.matrix, params);
LL<-bgbb.rf.matrix.LL(params, rf.matrix);
p.matrix.row<-c  (params, LL);
p.matrix<-rbind(p.matrix, p.matrix.row);
}
colnames(p.matrix)<-c("alpha","beta","gamma","delta","LL");
rownames(p.matrix)<-1:3;
p.matrix
bgbb.PlotTransactionRateHeterogeneity(params)
bgbb.PlotDropoutHeterogeneity(params)
bgbb.Expectation(params,n=10);
bgbb.Expectation(params,n=10)
n.cal=6
n.star=10
x=0
n.cal=6
n.star=10
x=0
t.x=0
bgbb.ConditionalExpectedTransactions(params, n.cal,
n.star, x, t.x)
x=4
t.x=5
bgbb.ConditionalExpectedTransactions(params, n.cal,
n.star, x, t.x)
histogram.bgbb.PlotFrequencyInCalibration(params, rf.matrix)
n.cal=6
n.star=10
x=0
t.x=0
bgbb.ConditionalExpectedTransactions(params, n.cal,
n.star, x, t.x)
# customer B
x=4
t.x=5
bgbb.ConditionalExpectedTransactions(params, n.cal,
n.star, x, t.x)
### As expected, B’s conditional expectation is much higher than A’s.
### The point I am trying to make, however, is that there are 3464 A’s in this dataset
### and only 284 B’s—you should never ignore the zeroes in these models
### Step 4: Plotting/ Goodness-of-fit
## the first plot to test the goodness-of-fit: a simple calibration period
histogram.bgbb.PlotFrequencyInCalibration(params, rf.matrix)
holdout.cbs<-simData$holdout$cbs
x.star<-holdout.cbs[, "x.star"]
holdout.cbs<-simData$holdout$cbs
x.star<-holdout.cbs[, "x.star"]
n.star<-5 # length of the holdout period
x.star<-donationsSummary$x.star
comp<-bgbb.PlotFreqVsConditionalExpectedFrequency(params, n.star,
rf.matrix, x.star)
rownames
(comp)<-c("act","exp","bin")
comp
comp<-bgbb.PlotRecVsConditionalExpectedFrequency(params, n.star,
rf.matrix, x.star)
rownames(comp)<-c("act","exp","bin")
comp
inc.track.data<-donationsSummary$annual.trans
n.cal<-6
xtickmarks<-1996:2006
inc.tracking<-bgbb.PlotTrackingInc(params, rf.matrix,inc.track.data, xticklab = xtickmarks)
rownames(inc.tracking)<-c("act","exp")
inc.tracking
cum.track.data<-cumsum(inc.track.data)
cum.tracking<-bgbb.PlotTrackingCum(params, rf.matrix, cum.track.data,xticklab= xtickmarks)
rownames(cum.tracking)<-c("act","exp")
cum.tracking
library(BTYD)
### STEP 1: data preparation
simElog<- system.file("data/discreteSimElog.csv", package="BTYD")
elog<-dc.ReadLines(simElog,cust.idx=1,date.idx= 2)
elog[1: 3,]
elog$date<- as.Date(elog$date,"%Y-%m-%d")
max(elog$date);
min(elog$date);
# let's make the calibration period end somewhere in-between
T.cal<-  as.Date("1977-01-01")
simData<-dc.ElogToCbsCbt(elog,per="year", T.cal)
cal.cbs<- simData$cal$cbs
freq<-cal.cbs[, "x"]
rec<-cal.cbs[,"t.x"]
trans.opp<- 7
# transaction opportunities
cal.rf.matrix<- dc.MakeRFmatrixCal(freq, rec, trans.opp)
cal.rf.matrix[1:5,]
### STEP 2: Parameter Estimation
### Estimating BG/BB parameters is very similar to estimating Pareto/NBD parameters
data(donationsSummary);
rf.matrix<-donationsSummary$rf.matrix
params<-bgbb.EstimateParameters(rf.matrix);
LL<-bgbb.rf.matrix.LL(params, rf.matrix);
p.matrix<-c(params, LL);
for(i in 1: 2) {
params<-bgbb.EstimateParameters(rf.matrix, params);
LL<-bgbb.rf.matrix.LL(params, rf.matrix);
p.matrix.row<-c  (params, LL);
p.matrix<-rbind(p.matrix, p.matrix.row);
}
colnames(p.matrix)<-c("alpha","beta","gamma","delta","LL");
rownames(p.matrix)<-1:3;
p.matrix;
### The parameter estimation converges very quickly. It is much easier,and faster,
### to estimate BG/BB parameters than it is to estimate Pareto/NBD parameters,because there are fewer calculations involved.
### We can interpret these parameters by plotting the mixing distributions.
### Alpha and beta describe the beta mixing distribution of the beta-Bernoulli transaction
### process. We can see the beta distribution by using
bgbb.PlotTransactionRateHeterogeneity(params)
### Gamma and Delta describe the beta mixing distribution of the beta-geometric
### dropout process. We can see the beta distribution with parameters gamma and delta using
bgbb.PlotDropoutHeterogeneity(params) # ERROR!!!!
#bgbb.PlotDropoutHeterogeneity(params) # ERROR!!!!
### The story told by these plots describes the type of customers most firms would
### want—their transaction parameters are more likely to be high, and their dropout parameters are more likely to be low.
### Step 3: Individual Level Estimations
### We can estimate the number of transactions we expect a newly acquired
### customer to make in a given time period, just as with the Pareto/NBD model
bgbb.Expectation(params,n=10);
## But we want to be able to say something about our existing customers, not
## just about a hypothetical customer to be acquired in the future
# customer A
n.cal=6
n.star=10
x=0
t.x=0
bgbb.ConditionalExpectedTransactions(params, n.cal,
n.star, x, t.x)
# customer B
x=4
t.x=5
bgbb.ConditionalExpectedTransactions(params, n.cal,
n.star, x, t.x)
### As expected, B’s conditional expectation is much higher than A’s.
### The point I am trying to make, however, is that there are 3464 A’s in this dataset
### and only 284 B’s—you should never ignore the zeroes in these models
### Step 4: Plotting/ Goodness-of-fit
## the first plot to test the goodness-of-fit: a simple calibration period
# histogram.bgbb.PlotFrequencyInCalibration(params, rf.matrix) ## Returns an error
## The next step is to see how well the model performs in the holdout period
holdout.cbs<-simData$holdout$cbs
x.star<-holdout.cbs[, "x.star"]
n.star<-5 # length of the holdout period
x.star<-donationsSummary$x.star
comp<-bgbb.PlotFreqVsConditionalExpectedFrequency(params, n.star,
rf.matrix, x.star)
rownames
(comp)<-c("act","exp","bin")
comp
## Since the BG/BB model uses discrete data, we can also bin customers by recency.
comp<-bgbb.PlotRecVsConditionalExpectedFrequency(params, n.star,
rf.matrix, x.star)
rownames(comp)<-c("act","exp","bin")
comp
### Another plotting function provided by the BTYD package is the tracking
### function—binning the data according to which time period transactions occurred in.
inc.track.data<-donationsSummary$annual.trans
n.cal<-6
xtickmarks<-1996:2006
inc.tracking<-bgbb.PlotTrackingInc(params, rf.matrix,inc.track.data, xticklab = xtickmarks)
rownames(inc.tracking)<-c("act","exp")
inc.tracking
###  smooth it out for a cleaner graph by making it cumulative
cum.track.data<-cumsum(inc.track.data)
cum.tracking<-bgbb.PlotTrackingCum(params, rf.matrix, cum.track.data,xticklab= xtickmarks)
rownames(cum.tracking)<-c("act","exp")
cum.tracking
#### Pareto / NBD
### The Pareto/NBD model is used for non-contractual situations in which customers
### can make purchases at any time. Using four parameters, it describes the rate
### at which customers make purchases and the rate at which they drop out—
### allowing for heterogeneity in both regards, of course. We will walk through the
###Pareto/NBD functionality provided by the BTYD package using the CDNOW1dataset.
library(BTYD)
### STEP 1: data preparation
## Step 1A: create an event log from the file “cdnowElog.csv”, which has customer IDs in the
## second column, dates in the third column and sales numbers in the fifth column
cdnowElog <- system.file("data/cdnowElog.csv", package = "BTYD")
elog <- dc.ReadLines(cdnowElog, cust.idx = 2, date.idx = 3, sales.idx = 5)
elog[1:3,]
## Step 1B: we convert the dates in the event log to R Date objects
elog$date <- as.Date(elog$date, "%Y%m%d")
elog[1:3,]
## Step 1C: The dc.MergeTransactionsOnSameDate function returns an event log
## with only one transaction per customer per day, with the total sum of
## their spending for that day as the sales number.
elog <- dc.MergeTransactionsOnSameDate(elog)
elog[1:3,]
## Step 1D: we need to divide the data up into a
## calibration period and a holdout period
## 30 September 1997 as the cutoff date,
## as this point (39 weeks)divides the dataset in half
end.of.cal.period <- as.Date("1997-09-30")
elog.cal <- elog[which(elog$date <= end.of.cal.period), ]
## Step 1E:  we use dc.SplitUpElogForRepeatTrans, which returns a filtered event
## log ($repeat.trans.elog) as well as saving important information about
## each customer ($cust.data)
split.data <- dc.SplitUpElogForRepeatTrans(elog.cal);
clean.elog <- split.data$repeat.trans.elog
## Step 1F: create a customer-by-time matrix
## This is simply a matrix with a row for each customer and a column for each date
freq.cbt <- dc.CreateFreqCBT(clean.elog);
freq.cbt[1:3,1:5]
## Step 1G: we create a customer-by-timematrix using all transactions,
## and then merge the filtered CBT with this total
## CBT (using data from the filtered CBT and customer IDs from the total CBT)
tot.cbt <-dc.CreateFreqCBT(elog)
cal.cbt<-dc.MergeCustomers(tot.cbt, freq.cbt)
## Step 1H:  we can finally create the customer-by-sufficient-statistic matrix described earlier
birth.periods<-split.data$cust.data$birth.per
last.dates<-split.data$cust.data$last.date
cal.cbs.dates<-data.frame(birth.periods, last.dates,end.of.cal.period)
cal.cbs<-dc.BuildCBSFromCBTAndDates(cal.cbt, cal.cbs.dates,per="week")
### Step 2: Model Parameter Estimation
params<-pnbd.EstimateParameters(cal.cbs);
params
LL<-pnbd.cbs.LL(params, cal.cbs);
LL
## As with any optimization, we should not be satisfied with the first output
## weget. Let’s run it a couple more times, with its own output as a starting
## point, to see if it converges:
p.matrix<-c(params, LL);
for (i in 1:2) {
params <- pnbd.EstimateParameters (cal.cbs, params);
LL <- pnbd.cbs.LL (params, cal.cbs);
p.matrix.row <- c(params, LL);
p.matrix <- rbind(p.matrix, p.matrix.row);
}
colnames(p.matrix) <- c("r", "alpha", "s", "beta", "LL");
rownames(p.matrix) <- 1:3;
p.matrix
## Now that we have the parameters, the BTYD package provides functions to
## interpret them
## As we know, r and alpha describe the gamma mixing distribution
## of the NBD transaction process. We can see this gamma distribution in figure2, plotted using
## pnbd.PlotTransactionRateHeterogeneity(params)
pnbd.PlotTransactionRateHeterogeneity(params)
## We also know that s and beta describe the gamma mixing distribution of the Pareto
##(or gamma exponential) dropout process. We can see this gamma distribution
## in figure 3, plotted using pnbd.PlotDropoutHeterogeneity(params)
#pnbd.PlotDropoutHeterogeneity(params)  ## Returns an error!!!!
### Step 3: Individual Level Estimations
## Now that we have parameters for the population, we can make estimations for
## customers on the individual level
## First, we can estimate the number of transactions we expect a newly acquired
## customer to make in a given time period. Let’s say, for example, that we are
## interested in the number of repeat transactions a newly acquired customer will
## make in a time period of one year.
pnbd.Expectation(params,t=52);
## We can also obtain expected characteristics for a specific customer,
## conditional on their purchasing behavior during the calibration period.
## The first of these is pnbd.ConditionalExpectedTransactions,
## which gives the number of transactions we expect a customer to make in the holdout period.
cal.cbs["1516",]
x<- cal.cbs["1516", "x"]
t.x <- cal.cbs["1516", "t.x"]
T.cal <-cal.cbs["1516", "T.cal"]
pnbd.ConditionalExpectedTransactions(params, T.star = 52, x, t.x, T.cal)
## The second is pnbd.PAlive,
## which gives the probability that a customer is still alive at the
## end of the calibration period.
pnbd.PAlive(params, x, t.x, T.cal)
## As above, the time periods used depend on which time period was used to estimate the parameters
## There is one more point to note here—using the conditional expectationfunction,
## we can see the “increasing frequency paradox” in action:
for (i in seq( 10, 25, 5)) {
cond.expectation <- pnbd.ConditionalExpectedTransactions(params, T.star = 52,
x = i, t.x = 20,  T.cal =39)
cat("x:" ,i," \t Expectation:", cond.expectation, fill = TRUE )
}
### Step 4: Plotting/ Goodness-of-fit
## We would like to be able to do more than make inferences about individual customers.
## The BTYD package provides functions to plot expected customer behavior against
## actual customer behaviour in the both the calibration and holdout periods
## The first such function is the obvious starting point: a comparison of actual
## and expected frequencies within the calibration period generated using the following code:
pnbd.PlotFrequencyInCalibration(params, cal.cbs,7)
## We need to verify that the fit of the model holds into the holdout period
elog<-  dc.SplitUpElogForRepeatTrans(elog)$repeat.trans.elog;
x.star<-rep(0,nrow(cal.cbs));
cal.cbs<-cbind(cal.cbs, x.star);
elog.custs<-elog$cust;
for(i in 1 : nrow (cal.cbs)) {
current.cust  <- rownames(cal.cbs)[i]
tot.cust.trans <- length( which(elog.custs == current.cust))
cal.trans <- cal.cbs[i, "x"]
cal.cbs[i, "x.star"]  <- tot.cust.trans - cal.trans
}
cal.cbs[1 : 3,]
## Now we can see how well our model does in the holdout period
T.star<-  39 # length of the holdout period
censor<-7 # This censor serves the same purpose described above
x.star<-cal.cbs[, "x.star"]
comp <- pnbd.PlotFreqVsConditionalExpectedFrequency(params, T.star, cal.cbs, x.star, censor)
rownames(comp)<-c("act","exp","bin")
comp
## how well does our model predict how many transactions will occur in each week?
tot.cbt<-  dc.CreateFreqCBT(elog)
d.track.data<-rep(0,7*78)
origin<-as.Date("1997-01-01")
for(i in colnames(tot.cbt)) {
date.index  <-  difftime( as.Date(i), origin) + 1;
d.track.data[date.index]<-sum(tot.cbt[,i]);
}
w.track.data<-rep(0,78)
for(j in 1 : 78){
w.track.data[j] <-sum(d.track.data[(j * 7 -6) : (j * 7)])
}
## Now, we can make a plot comparing the actual number of transactions
## to the expected number of transactions on a weekly basis
T.cal<- cal.cbs[, "T.cal"]
T.tot<- 78
n.periods.final<- 78
inc.tracking<-pnbd.PlotTrackingInc(params, T.cal,
T.tot, w.track.data,
n.periods.final)
inc.tracking[,20:25]
## We may need to smooth the data out by cumulating it over time
cum.tracking.data<- cumsum(w.track.data)
cum.tracking<-pnbd.PlotTrackingCum(params, T.cal,
T.tot, cum.tracking.data,
n.periods.final)
cum.tracking[, 20:25]
cum.tracking[, 20:25]
cum.tracking[, 20:25]
cum.tracking[, 20:25]
